{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "     #print 'numerator',numerator\n",
    "     #print 'dinominator',denominator\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "    \n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sensecosine(word,word1):\n",
    "    documents=(word,word1)\n",
    "\n",
    "    tfidf_vectorizer=TfidfVectorizer(analyzer=\"char\")\n",
    "    tfidf_matrix=tfidf_vectorizer.fit_transform(documents)\n",
    "    #print tfidf_matrix.shape\n",
    "\n",
    "    cs=cosine_similarity(tfidf_matrix[0-1],tfidf_matrix)\n",
    "    #print cs.min()\n",
    "    return cs.min()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createnewstr1(val):    \n",
    "    smalltok =[\"360\",\"coaching\",\"agile\",\"payroll\",\"policies\",\"risk\",\"evp\",\"email\",\"vendor\",\"learning\",\"charter\",\"Webinar\",\"juniper\",\"templates\",\"webinars\",\"talent\",\"hrbp\",\"metrics\",\"benefits\",\"digital\",\"kpi\",\"sox\",\"cloud\",\"culture\",\"Analytics\",\"attrition\",\"mentoring\",\"PMO\",\"change\",\"video\",\"ERP\",\"budget\",\"it\",\"diversity\",\"career\",\"HRIS \",\"dashboard\",\"SAP\",\"retention\",\"roadmap\",\"values\",\"feedback\",\"mobility\",\"turnover\",\"reporting\",\"m&a\",\"interview\",\"security\",\"sourcing\",\"ethics\",\"maturity\",\"HIPO\",\"IDP\",\"mobilizer\",\"elearning\",\"fraud\",\"training\",\"marketing\",\"privacy\",\"Cisco\",\"HRIS\",\"finance\",\"goals\",\"RFP\",\"travel\",\"ERM\",\"IT \",\"merger\",\"survey\",\"quality\",\"seagate\",\"audit\",\"tools\",\"planning\",\"Hiring\",\"byod\",\"process\",\"INDUCTION\",\"strategy\",\"cargill\",\"devops\",\"Mergers\",\"ignition\",\"scorecard\",\"CRM\",\"playbook\",\"tool\",\"RACI\",\"ITIL\",\"trends\",\"excel\",\"influence\",\"Mentor\",\"cpe\",\"wellness\",\"sla\",\"lean\",\"promotion\",\"SWOT\",\"severance\",\"intranet\",\"research\",\"template\",\"shell\",\"HR\",\"policy\",\"china\",\"budgeting\",\"checklist\",\"a\",\"9 box \",\"tax\",\"mobile\",\"Data\",\"SAP \",\"pricing\",\"GRC\",\"policy \",\"treasury\",\"9 box\",\"loyalty\",\"mentor \",\"Training \",\"brand\",\"roi\",\"lms\",\"big data\",\"benchmark\",\"branding\"]\n",
    "    #print type(a)\n",
    "    bigtok = [\"conflict\",\"interest\",\"performance\",\"management\",\"competencies\",\"onboarding\",\"Interview\",\"guide\",\"cybersecurity\",\"career\",\"pathing\",\"strategy\",\"page\",\"CEB\",\"IGNITION\",\"competency\",\"business\",\"continuity\",\"Job\",\"descriptions\",\"leadership\",\"development\",\"Innovation\",\"performance\",\"improvement\",\"plan\",\"recruiting\",\"chief\",\"staff\",\"innovation\",\"capability\",\"outsourcing\",\"root\",\"cause\",\"analysis\",\"recognition\",\"customer\",\"experience\",\"vendor\",\"management\",\"integration\",\"collaboration\",\"technology\",\"e-learning\",\"millenials\",\"exit\",\"interview\",\"recruitment\",\"Data\",\"Analytics\",\"root\",\"cause\",\"customer\",\"service\",\"benchmarking\",\"hr\",\"business\",\"partner\",\"career\",\"path\",\"new\",\"hire\",\"survey\",\"career\",\"development\",\"Learning\",\"and\",\"Development\",\"span\",\"control\",\"employee\",\"value\",\"proposition\",\"Data\",\"Governance\",\"disaster\",\"recovery\"]\n",
    "    line = val\n",
    "    tokens = line.split()\n",
    "    present1 = []\n",
    "    present2 = []\n",
    "    #print present\n",
    "    #print b\n",
    "    df = pd.DataFrame( columns=['A','B'])\n",
    "    for word in tokens:\n",
    "        if filter(lambda x: word in x, smalltok):\n",
    "            smalltokfound1= filter(lambda x: word in x, smalltok)\n",
    "            #print \"this\"\n",
    "            #print smalltokfound1\n",
    "            st1 = smalltokfound1.pop()\n",
    "            if word==st1:\n",
    "                present1.insert(0,st1)\n",
    "                break;\n",
    "                #print present1\n",
    "                #here we add the actual token with the score\n",
    "        else:\n",
    "            for word1 in bigtok:\n",
    "                \n",
    "                vector1 = text_to_vector(word)\n",
    "                vector2 = text_to_vector(word1)\n",
    "                cosine = sensecosine(word, word1)\n",
    "                if cosine>0.8:\n",
    "                    present2.append(word1)\n",
    "                    \n",
    "                \n",
    "            #bigtokfound1 = filter(lambda x: word in x, bigtok)\n",
    "            #st2 = bigtokfound1.pop()\n",
    "            #if word==st2:\n",
    "                #present2.append(st2)\n",
    "\n",
    "                #print present2\n",
    "\n",
    "\n",
    "            #print \"not in small tok\"\n",
    "    newstr = \"\"\n",
    "    for word in present2:\n",
    "        newstr = newstr+\" \"+word\n",
    "\n",
    "    for word in present1:\n",
    "        newstr = newstr+\" \"+word\n",
    "    #print newstr\n",
    "    return newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/classifyTokens.csv\",\n",
    "                sep=\",\", names=[\"search\",\"matchword\"],\n",
    "                skiprows=1)\n",
    "data= df\n",
    "df.head()\n",
    "df2 = pd.DataFrame( columns=['search','matchword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['performance management', ' dgsugdf', 'gjdgsjg', 'askdgkjd', 'vcsvcn']\n",
      "['performance management', ' dgsugdf', 'gjdgsjg', 'askdgkjd', 'vcsvcn']\n"
     ]
    }
   ],
   "source": [
    "def getsearchtokens(val):    \n",
    "    j=0\n",
    "    newstr = createnewstr1(val)\n",
    "    try:    \n",
    "        for i in range(0, len(df)):\n",
    "            sdata = df.iloc[i]['search']\n",
    "\n",
    "            smatch = df.iloc[i]['matchword']\n",
    "            #sdata = 'performance management'\n",
    "            vector1 = text_to_vector(newstr)\n",
    "            vector2 = text_to_vector(sdata)\n",
    "            #print 'vector1',vector1\n",
    "            cosine = get_cosine(vector1, vector2)\n",
    "            #print newstr\n",
    "            #print type(cosine)\n",
    "            cos = int(cosine)\n",
    "            #print type(cos)\n",
    "            #print cos\n",
    "            if cosine>0.8:\n",
    "                df2.loc[j] = [sdata ,smatch]\n",
    "                j=j+1\n",
    "        matchw = \"\"\n",
    "        for i in range(0, len(df2)):\n",
    "            #print df2.iloc[i]['search'],\"    \", df2.iloc[i]['matchword']\n",
    "            matchw= str(df2.iloc[i]['matchword'])\n",
    "            matchw1 = matchw.split(\",\")\n",
    "            print matchw1\n",
    "            break;\n",
    "        return matchw1\n",
    "    except:\n",
    "        return []\n",
    "mat = getsearchtokens(\"i wanna know abt perfarmance mangement\")\n",
    "print mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [A, B]\n",
      "Index: []\n",
      "                      A        B\n",
      "0  conflict of interest  123.234\n",
      "conflict of interest 123.234\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame( columns=['A','B'])\n",
    "print df\n",
    "df.loc[0] = [\"conflict of interest\",123.234]\n",
    "print df\n",
    "for i in range(0, len(df)):\n",
    "    print df.iloc[i]['A'], df.iloc[i]['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    conflict of interest\n",
      "B                 123.234\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for row in df.iterrows():\n",
    "    print row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createnewstr(val):    \n",
    "    smalltok =[\"360\",\"coaching\",\"agile\",\"payroll\",\"policies\",\"risk\",\"evp\",\"email\",\"vendor\",\"learning\",\"charter\",\"Webinar\",\"juniper\",\"templates\",\"webinars\",\"talent\",\"hrbp\",\"metrics\",\"benefits\",\"digital\",\"kpi\",\"sox\",\"cloud\",\"culture\",\"Analytics\",\"attrition\",\"mentoring\",\"PMO\",\"change\",\"video\",\"ERP\",\"budget\",\"it\",\"diversity\",\"career\",\"HRIS \",\"dashboard\",\"SAP\",\"retention\",\"roadmap\",\"values\",\"feedback\",\"mobility\",\"turnover\",\"reporting\",\"m&a\",\"interview\",\"security\",\"sourcing\",\"ethics\",\"maturity\",\"HIPO\",\"IDP\",\"mobilizer\",\"elearning\",\"fraud\",\"training\",\"marketing\",\"privacy\",\"Cisco\",\"HRIS\",\"finance\",\"goals\",\"RFP\",\"travel\",\"ERM\",\"IT \",\"merger\",\"survey\",\"quality\",\"seagate\",\"audit\",\"tools\",\"planning\",\"Hiring\",\"byod\",\"process\",\"INDUCTION\",\"strategy\",\"cargill\",\"devops\",\"Mergers\",\"ignition\",\"scorecard\",\"CRM\",\"playbook\",\"tool\",\"RACI\",\"ITIL\",\"trends\",\"excel\",\"influence\",\"Mentor\",\"cpe\",\"wellness\",\"sla\",\"lean\",\"promotion\",\"SWOT\",\"severance\",\"intranet\",\"research\",\"template\",\"shell\",\"HR\",\"policy\",\"china\",\"budgeting\",\"checklist\",\"a\",\"9 box \",\"tax\",\"mobile\",\"Data\",\"SAP \",\"pricing\",\"GRC\",\"policy \",\"treasury\",\"9 box\",\"loyalty\",\"mentor \",\"Training \",\"brand\",\"roi\",\"lms\",\"big data\",\"benchmark\",\"branding\"]\n",
    "    #print type(a)\n",
    "    bigtok = [\"conflict\",\"interest\",\"performance\",\"management\",\"competencies\",\"onboarding\",\"Interview\",\"guide\",\"cybersecurity\",\"career\",\"pathing\",\"strategy\",\"page\",\"CEB\",\"IGNITION\",\"competency\",\"business\",\"continuity\",\"Job\",\"descriptions\",\"leadership\",\"development\",\"Innovation\",\"performance\",\"improvement\",\"plan\",\"recruiting\",\"chief\",\"staff\",\"innovation\",\"capability\",\"outsourcing\",\"root\",\"cause\",\"analysis\",\"recognition\",\"customer\",\"experience\",\"vendor\",\"management\",\"integration\",\"collaboration\",\"technology\",\"e-learning\",\"millenials\",\"exit\",\"interview\",\"recruitment\",\"Data\",\"Analytics\",\"root\",\"cause\",\"customer\",\"service\",\"benchmarking\",\"hr\",\"business\",\"partner\",\"career\",\"path\",\"new\",\"hire\",\"survey\",\"career\",\"development\",\"Learning\",\"and\",\"Development\",\"span\",\"control\",\"employee\",\"value\",\"proposition\",\"Data\",\"Governance\",\"disaster\",\"recovery\"]\n",
    "    line = val\n",
    "    tokens = line.split()\n",
    "    present1 = []\n",
    "    present2 = []\n",
    "    #print present\n",
    "    #print b\n",
    "    df = pd.DataFrame( columns=['A','B'])\n",
    "    for word in tokens:\n",
    "        if filter(lambda x: word in x, smalltok):\n",
    "            smalltokfound1= filter(lambda x: word in x, smalltok)\n",
    "            #print \"this\"\n",
    "            #print smalltokfound1\n",
    "            st1 = smalltokfound1.pop()\n",
    "            if word==st1:\n",
    "                present1.insert(0,st1)\n",
    "                break;\n",
    "                #print present1\n",
    "                #here we add the actual token with the score\n",
    "        elif filter(lambda x: word in x, bigtok):\n",
    "            bigtokfound1 = filter(lambda x: word in x, bigtok)\n",
    "            st2 = bigtokfound1.pop()\n",
    "            if word==st2:\n",
    "                present2.append(st2)\n",
    "\n",
    "                #print present2\n",
    "\n",
    "\n",
    "            #print \"not in small tok\"\n",
    "    newstr = \"\"\n",
    "    for word in present2:\n",
    "        newstr = newstr+\" \"+word\n",
    "\n",
    "    for word in present1:\n",
    "        newstr = newstr+\" \"+word\n",
    "    #print newstr\n",
    "    return newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' performance management competencies'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createnewstr(\"i want to know about performance management and competencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vector1 = text_to_vector('performance')\n",
    "vector2 = text_to_vector('perform')\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "print cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensecosine(\"performance\",\"parformance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "positive = [\"yes\",\"yeah\",\"thank you\",\"thanks\"]\n",
    "negative = [\"no\",\"not\",\"not sure\",\"more\"]\n",
    "def ispositive(val):\n",
    "    flaggy = '0'\n",
    "    for word1 in positive:\n",
    "        if sensecosine(val,word1)>0.7:\n",
    "            flaggy = 'pos';\n",
    "            \n",
    "    for word2 in negative:\n",
    "        if sensecosine(val,word2)>0.7:\n",
    "            flaggy = 'neg';\n",
    "            \n",
    "    return flaggy       \n",
    "            \n",
    "\n",
    "        \n",
    "         \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  st\n",
      "1  nm\n",
      "2  st\n"
     ]
    }
   ],
   "source": [
    "st = \"1\"\n",
    "print st,\" st\"\n",
    "nm = int(st)\n",
    "print nm,\" nm\"\n",
    "nm = nm+1\n",
    "st = str(nm)\n",
    "print st,\" st\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-4eccb91e62fd>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-4eccb91e62fd>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    toki = int(intro)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#dff = pd.DataFrame( columns=['match','setval'])\n",
    "    #dff = getdff()\n",
    "    toki = int(intro)\n",
    "    if  intro ==\"intro over\":\n",
    "        \n",
    "        if toki<len(dff):\n",
    "            match1 = dff.iloc[toki]['match']\n",
    "            dff.loc[toki] = [match1 ,val]\n",
    "            toki=toki+1\n",
    "            match2 = dff.iloc[toki]['match']\n",
    "            ans = \"would you like to give me more information on \",match2\n",
    "            intro = str(toki)\n",
    "            #if toki==len(dff):\n",
    "               # match2 = dff.iloc[toki]['match']\n",
    "               # ans = \"would you like to give me more information on \",match2\n",
    "               # intro = \"introa\"\n",
    "            #else:\n",
    "                #match2 = dff.iloc[toki]['match']\n",
    "                #ans = \"would you like to give me more information on \",match2\n",
    "                #intro=\"introa\"\n",
    "                \n",
    "        elif toki==len(dff):\n",
    "            flag = '1'\n",
    "            intro = \"intro over\"\n",
    "            ans = \"query the first elastic search\"\n",
    "            \n",
    "            #return json.dumps({'answer':ans,'flag':flag,'feedback':feedback})\n",
    "            \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "#import pyorient\n",
    "import ConfigParser\n",
    "import requests\n",
    "import os,sys\n",
    "#from LoggingModule import *\n",
    "import re, collections\n",
    "import sys\n",
    "reload(sys)\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "def elastic(val):\n",
    "      try:\n",
    "        print \"val reach\"\n",
    "        userInput = val\n",
    "       \n",
    "         \n",
    "               \n",
    "        response = requests.get(val)   \n",
    "        \n",
    "        val = response.json()\n",
    "        res = val.items()\n",
    "        #print res \n",
    "        val = response.json()\n",
    "        #print val\n",
    "        try:\n",
    "             elastic = val['hits']['hits']\n",
    "             \n",
    "             elastic1 = elastic[0]\n",
    "             elastic2 = str(elastic1)\n",
    "             elastic3 = json.dumps(elastic2)  \n",
    "             elastic4 = json.loads(elastic3)\n",
    "#              elastic5 = elastic4.json()\n",
    "             #elastic6 = elastic5['_score']\n",
    "             #elastic2 = elastic[1:len(elastic1)-2]\n",
    "             #elastic = elastic2.json()\n",
    "            \n",
    "        except : \n",
    "            \n",
    "             elastic = \"0\"\n",
    "      except:\n",
    "           print \"Cannot connect to service\"\n",
    "      \n",
    "      print elastic2\n",
    "      matchObj = re.match( r'(.*)data\\'\\: \\[u\\'(.*)\\'\\]', elastic2, re.M|re.I)\n",
    "      if matchObj:\n",
    "            print \"matchObj.group(1) : \", matchObj.group(1)\n",
    "            name = matchObj.group(2)\n",
    "      dff = pd.DataFrame( columns=['mat','setval'])  \n",
    "      dff.loc[0] = [\"I have found this document where topics of your interest might be there \" ,\" Is this what you were looking for ?\"]\n",
    "      dff.loc[1] = [\"With the search requirements you have given, I would like to prefer this document \" ,\" Is this document of any use?\"]\n",
    "      dff.loc[2] = [\"I think you are looking for this document \" ,\" Aren't you?\"]\n",
    "      dff.loc[3] = [\"This document is most viewed by many members \" ,\" I thinks you got your document, didn't you?\"]\n",
    "      i = randint(0,3)\n",
    "      matchi =  str(dff.iloc[i]['mat'])\n",
    "      setvi = str(dff.iloc[i]['setval'])\n",
    "      strlink = \"http://cebglobal.com/gdkfgkdg/fhdjsgfkg\"\n",
    "      ans = matchi+\"\\n\"+\"\\\"\"+name+\"\\\"\\n\"+setvi\n",
    "      print \"-sss\"\n",
    "#       print elastic5\n",
    "      print \"-------\" \n",
    "      return ans\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val= elastic(\"http://localhost:9200/hack1/_search?q=keyword:cybersecurity AND cat:1&fields=data&from=1&size=1\")\n",
    "print str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ll = [\"khskch\",\"bjcgjvasdj\"]\n",
    "print str(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
